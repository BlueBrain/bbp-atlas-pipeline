image: python:3.10

include:
  - project: dke/apps/templates/job-templates
    file: job-templates.yml
  - project: cs/gitlabci-templates
    file: /build-image-using-kaniko.yml

stages:
  - unit-test
  - update-dag
  - nexus_synchronization
  - deploy_image
  - convert_and_deployToBB5
  - generate_doc
  - deploy_doc
  - run-pipeline

variables:
  DOC_DIR: "doc"
  DAG_DIR: "$DOC_DIR/source/figures"
  DOC: "generated/html"
  DEV_BRANCH: "develop"
  REPO_PUSH_TOKEN: $OAUTH2_TOKEN
  COMMIT_SHA: $CI_COMMIT_SHA
  FINAL_PIPELINE_STEP: "push_atlas_datasets"
  PROJ84_GPFS: "/gpfs/bbp.cscs.ch/data/project/proj84"
  IMAGES_DIR: "$PROJ84_GPFS/atlas_singularity_images"
  PIPELINE_RUNS: "$PROJ84_GPFS/atlas_pipeline_runs"
  TIMESTAMP: "$CI_COMMIT_TIMESTAMP"
  IMAGE_PATH: "$IMAGES_DIR/${CI_PROJECT_NAME}_${REGISTRY_IMAGE_TAG}-${TIMESTAMP}.sif"
  IMAGE_LINK: "${CI_PROJECT_NAME}_${REGISTRY_IMAGE_TAG}.sif"
  IMAGE_LINK_PATH: "${IMAGES_DIR}/${IMAGE_LINK}"
  TARGET_RULE: "$FINAL_PIPELINE_STEP"  # Useful to change it in the job manual execution
  SERVICE_TOKEN_SETTINGS: "SERVICE_TOKEN=True TOKEN_USERNAME=$SERVICE_TOKEN_USERNAME TOKEN_PASSWORD=$SERVICE_TOKEN_PASSWORD"


.deploy_rules:
  rules:
    - if: $CI_COMMIT_TAG != null
      when: on_success
      variables:
        CI_REGISTRY_IMAGE: $CI_REGISTRY_IMAGE
        REGISTRY_IMAGE_TAG: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH == $DEV_BRANCH
      when: on_success
      variables:
        CI_REGISTRY_IMAGE: $CI_REGISTRY_IMAGE
        REGISTRY_IMAGE_TAG: dev

.add_staging_SSL:
  script:
    - CA_BUNDLE=$(python3 -c "import certifi; print(certifi.where())")
    - echo "$BBP_CA_CERT" >> $CA_BUNDLE
    - export SSL_CERT_FILE=$CA_BUNDLE

.git_setup:
  - git remote add gitlab https://ci:$REPO_PUSH_TOKEN@bbpgitlab.epfl.ch/dke/apps/blue_brain_atlas_pipeline
  - git config user.name "$COMMIT_SHA"
  - git config user.email "bbp-ou-dke@groupes.epfl.ch"

.load_singularity_module:
  - echo "Load the singularity module:"
  - module load unstable singularityce


unit_test:
  stage: unit-test
  extends: .unit-tests
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
  before_script:
    - pip install -i https://bbpteam.epfl.ch/repository/devpi/simple/ .[dev]
    - export PYTHONPATH=.:$PYTHONPATH
  script:
    #- !reference [.add_staging_SSL, script]
    - !reference [.unit-tests, script]
  variables:
    SRC_PROJECT: '$CI_PROJECT_PATH'
    TEST_FOLDER: 'tests'
    NEXUS_STAGING_TOKEN: '$NEXUS_STAGING_TOKEN'
    SERVICE_TOKEN_SETTINGS: '$SERVICE_TOKEN_SETTINGS'
    KUBERNETES_MEMORY_LIMIT: 8Gi
    KUBERNETES_MEMORY_REQUEST: 8Gi

update_dags:
  stage: update-dag
  rules:
    - changes:
        paths:
          - snakefile
        #compare_to: 'refs/heads/develop'
      when: manual
  before_script:
    - apt update
    - apt-get -y install graphviz
    - pip install -i https://bbpteam.epfl.ch/repository/devpi/simple/ .[dev]
    - !reference [.git_setup]
    - git checkout $CI_COMMIT_BRANCH
  script:
    - echo "Update DAG"
    - snakemake  --config SERVICE_TOKEN=True TOKEN_USERNAME=$SERVICE_TOKEN_USERNAME TOKEN_PASSWORD=$SERVICE_TOKEN_PASSWORD  --dag  $FINAL_PIPELINE_STEP > $DAG_DIR/dag_push_atlas.gv
    - dot -Tsvg $DAG_DIR/dag_push_atlas.gv > $DAG_DIR/dag_push_atlas.svg
    - echo "Update detailed DAG"
    - snakemake  --config SERVICE_TOKEN=True TOKEN_USERNAME=$SERVICE_TOKEN_USERNAME TOKEN_PASSWORD=$SERVICE_TOKEN_PASSWORD  --filegraph  $FINAL_PIPELINE_STEP > $DAG_DIR/dag_push_atlas_fg.gv
    - dot -Tsvg $DAG_DIR/dag_push_atlas_fg.gv > $DAG_DIR/dag_push_atlas_fg.svg
    - echo "Commit updated DAGs"
    - git add $DAG_DIR/dag_push_atlas*
    - git diff-index --quiet HEAD  ||  git commit -m "Update DAGs"
    - git push -o ci.skip gitlab $CI_COMMIT_BRANCH
  allow_failure: true


# Update Nexus Resources
nexus-synchronization:
  stage: nexus_synchronization
  rules:
    - !reference [.deploy_rules, rules]
  variables:
    NEXUS_STAGING_TOKEN: '$NEXUS_STAGING_TOKEN'
    NEXUS_IDS_PATH: "nexus_ids.json"
    METADATA_DIR: "metadata"
    FILE_NEXUS_ID_MAP: "file_nexus_map.json"
  before_script:
    - !reference [.git_setup]
    - git checkout $DEV_BRANCH
  script:
    - pip install "nexusforge==0.8.1"
    - python synch_nexus.py
    - git add $NEXUS_IDS_PATH $METADATA_DIR/$FILE_NEXUS_ID_MAP
    - git diff-index --quiet HEAD  ||  git commit -m "Update $NEXUS_IDS_PATH"
    - git push -o ci.skip gitlab $DEV_BRANCH
  allow_failure: true

# Build image for pipeline
update-pipeline-image:
  stage: deploy_image
  extends: .build-image-using-kaniko
  rules:
    - !reference [.deploy_rules, rules]
  variables:
    BUILD_PATH: $CI_PROJECT_DIR
    KANIKO_EXTRA_ARGS: "--build-arg CI_JOB_TOKEN=$CI_JOB_TOKEN --build-arg BBP_CA_CERT='$BBP_CA_CERT'"
    KUBERNETES_MEMORY_LIMIT: 4Gi
    KUBERNETES_MEMORY_REQUEST: 4Gi
  before_script:
    - export IFS=''

# Generate documentation and stores the artifact under doc/generated/html
generate-documentation:
  stage: generate_doc
  rules:
      - if: $CI_COMMIT_TAG != null  || $CI_COMMIT_BRANCH == "master" || $CI_COMMIT_BRANCH == "develop" || $CI_MERGE_REQUEST_IID || $CI_COMMIT_MESSAGE =~ /DRAFT$/
        when: on_success
  script: sphinx-build -T --keep-going -b html -c ./$DOC_DIR/source -D language=en ./$DOC_DIR/source $DOC_DIR/$DOC
  before_script:
    - pip install -i https://bbpteam.epfl.ch/repository/devpi/simple/ -r requirements_doc.txt
  artifacts:
    paths:
      - $DOC_DIR/$DOC
  variables:
    KUBERNETES_MEMORY_LIMIT: 4Gi
    KUBERNETES_MEMORY_REQUEST: 4Gi

# Executes deployment of project documentation to bbp-dke-staging Openshift
deploy-documentation-in-registry:
  stage: deploy_doc
  extends: .build-image-using-kaniko
  dependencies:
    - generate-documentation
  rules:
      - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "master"'
        when: on_success
        variables:
          CI_REGISTRY_IMAGE: $CI_REGISTRY_IMAGE/sphinx-documentation-prod
      - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "develop"'
        when: on_success
        variables:
          CI_REGISTRY_IMAGE: $CI_REGISTRY_IMAGE/sphinx-documentation-dev
  variables:
    CI_COMMIT_SHORT_SHA: $CI_COMMIT_SHORT_SHA
    REGISTRY_IMAGE_TAG: $CI_COMMIT_SHORT_SHA-$(date +%s)
    BUILD_PATH: $CI_PROJECT_DIR/$DOC_DIR
    KANIKO_EXTRA_ARGS: "--build-arg CI_PROJECT_DIR=$CI_PROJECT_DIR  --build-arg DOC_DIR=$DOC_DIR  --build-arg DOC_PATH=$DOC"

# Convert Docker to Singularity image and deploy in BB5
convert_and_deploy_job:
  stage:
    convert_and_deployToBB5
  rules:
    - !reference [.deploy_rules, rules]
  needs:
    - update-pipeline-image
  when: on_success
  tags:
    - bb5_map
  variables:
    bb5_constraint: nvme
    bb5_cpus_per_task: 2
    bb5_memory: 4G
    bb5_duration: "10:00"
  before_script:
    - !reference [.load_singularity_module]
  script:
    - bash convert_singularity_image.sh

run_pipeline:
  stage:
    run-pipeline
  when: manual
  tags:
    - bb5_map
  before_script:
    - !reference [.load_singularity_module]
  script:
    - cat <<EOF | singularity shell  --mount "type=bind,source=$PROJ84_GPFS,destination=$PROJ84_GPFS"  $IMAGE_LINK_PATH
    - set -e
    - export PYTHONPATH=$PIPELINE_REPO:$PYTHONPATH
    - snakemake  --snakefile $CUSTOM_PIPELINE/custom_snakefile  --configfile $PIPELINE_REPO/config.yaml  --config TARGET_RULE=$TARGET_RULE USER_CONFIG=$CUSTOM_PIPELINE/user_config.json REPO_PATH=$PIPELINE_REPO $SERVICE_TOKEN_SETTINGS  --unlock   # in case a previous pipeline failed ("the remaining lock was likely caused by a kill signal or a power loss")
    - bbp-atlas  --target-rule $TARGET_RULE  --user-config-path $CUSTOM_PIPELINE/user_config.json  --repo-path $PIPELINE_REPO  --service-token  --token-username $SERVICE_TOKEN_USERNAME  --token-password $SERVICE_TOKEN_PASSWORD  --snakemake-options '--configfile $PIPELINE_REPO/config.yaml  --config $SERVICE_TOKEN_SETTINGS WORKING_DIR=$PIPELINE_OUTPUT NEXUS_REGISTRATION=False EXPORT_MESHES=False  --cores $SNAKEMAKE_CORES'
    - EOF
  after_script:
    if [ $CI_COMMIT_TAG != null ]; then
      ln -s $PIPELINE_OUTPUT $PIPELINE_RUNS/$CI_COMMIT_TAG
    fi
  timeout: 8 hours
  variables:
    PIPELINE_REPO: /pipeline/$CI_PROJECT_NAME  # set in the Dockerfile
    CUSTOM_PIPELINE: $PIPELINE_REPO/customize_pipeline
    PIPELINE_OUTPUT: $PIPELINE_RUNS/$TIMESTAMP
    SNAKEMAKE_CORES: 70
    bb5_cpus_per_task: 70
    bb5_memory: 0
    bb5_duration: "8:00:00"
    bb5_exclusive: full
