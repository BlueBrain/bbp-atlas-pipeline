# Blue Brain Atlas Pipeline
## What is this repository about?
The Blue Brain Atlas Pipeline (BBAP) is composed of two repositories:
- A repository about each individual module that the pipeline will/can run (find it [here](https://bbpcode.epfl.ch/code/#/admin/projects/project/proj84/blue_brain_atlas))
- This repository, that contains all the logic for a proper orchestrated execution of these modules

In order to guarantee an certain ease of use for the development team and the pipeline, portability is key, hence we made the choice to use [Snakemake](https://snakemake.readthedocs.io/en/stable/) as the backbone of the pipeline orchestration.

## General documentation
The BBAP is documented [here](https://bbpteam.epfl.ch/project/spaces/display/BBKG/Atlas+Pipeline). This will give an overview of the project as well as some precise technical details.

## Brief data overview
This pipeline is in charge of the orchestration of all the modules involved in the creation of the Atlas datasets at BBP, included but not limited to:
- Gene expression volumes (NRRD)
- Cell density volume (NRRD)
- Brain region annotation hybrid volume (NRRD)
- Orientation volume (NRRD)
- Brain region meshes (OBJ)
- Cell position and orientation file (HDF5)

This atlas relies on Allen Mouse CCF v2 and v3 dataset


# Task to make the Snakemake pipeline possible

- Check modules that donâ€™t comply with explicit I/O argument (some module take a working directory as unique argument and the code decides what to read from it and what to write in it. We no longer want this behavior) and that are not conformant with the pipeline guides. Provide feedback to modules owners [4 days]

- Write the pipeline module in charge of fetching data from Nexus - would probably require a service account to make that easier (in a second phase, we may develop a module for fetching a token with the Keycloak Python lib) [2 days]

- Run each module of the pipeline independently to perform a "human check" for I/O [4 days]

- Write and test the Snakemake orchestrating file (Snakefile) to run the whole pipeline, with I/O "connected" (e.g. stating that each input is also the output of another module) [5 days]

- Develop a module to test the integrity of generated data (e.g. NRRD files). This module will make sure the file contains all the information the format specification allows to describe the data accurately. It will also check the buffer of numerical data has the expected byte size. The integrity check must be perform in priority on datasets to be stored in Nexus but also on temporary data (generated by one module, used by another) to make sure the "garbage-in garbage-out" situation does not happen [3 days]

- Write the Nexus mappers for NRRD and mesh files (partly done, must be adapted for the pipeline purpose) [4 days]

- Write the pipeline module to push data into Nexus (mainly NRRD and meshes) [3 days]

- Write some documentation/tutorial about how to run the pipeline [2 days]

- Build the non regression test suite (compare volumes and mesh to a reference dataset, checksum, measure distances) [5 days]

total: 28-32 days
